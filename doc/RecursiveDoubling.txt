OBB: Recursive Doubling Allreduce Fault Tolerante 

## Report Algoritmo Recursive Doubling Fault Tolerante 
- Ogni processo conserva informazioni sui processi attivi e inattivi 
- Uso due comunicatori uno globale e uno per gli attivi
- Tutti i processi entrano nella computazione della recursive doubling 
- I processi attivi computano, gli altri osservano 
- All inizio di ogni passo ho una barriera che viene usata per notificare i fault [comm globale]
- Ad ogni passo prima di fare sendrecv vedo se ho avuto un errore al passo precedente 
- Se si chiamo errhandler
    - Controllo fault su comunicatore globale e lo aggiusto, ma mantengo il vecchio
        comunicatore globale [1.0]
    - Se è fallito un rank inattivo sistemo la struttura inactive_ranks e conto i rank 
        inattivi che sono falliti [2.0]
    - Se è fallito un rank attivo
        - Tutti i nodi controllano chi è fallito nel comunicatore attivo [3.0]
            - Se abbiamo abbastanza nodi inattivi [3.1]
                - Mi calcolo: 
                    - Chi deve svegliare inattivi e mandare dati ai rank corrotti [3.1.0]
                    - Chi è corrotto [3.1.1]
                    - Chi è inattivo e deve essere svegliato [3.1.2]
            - Altrimenti [3.2]
                - Mi calcolo la potenza di due minore più vicina [3.2.0]
                - Scelgo i rank che saranno attivi [3.2.1]
            - Creo nuovo comunicatore degli attivi [attraverso old_comm] [3.3]
            - Aggiusto i dati se necessario mandando i dati corretti ai processi
                che hanno i dati corrotti [3.4]
    - Alla fine 
        - Aggiusto la struct in base ai nodi falliti [4.0]
        - Poi calcolano se sono ancora un nodo inattivo o no [5.0]
- Continuo con la recursive doubling
- Quando finisce l'algoritmo
- Gli inattivi aspettano che un attivo gli manda il risultato [6.0]
- Gli attivi mandano il risultato agli inattivi corrispettivi [7.0]
- Alla fine tutti hanno il risultato corretto

## Note 
1.0 -> Mantengo il vecchio comunicatore cha ha fallito perchè i rank nella struttura data 
    hanno il valore relativo al vecchio comunicatore e viene aggiustato solo alla fine.
    Uso quindi il vecchio comunicatore per prendermi il gruppo dei rank quando quando Creo
    il nuovo comunicatore degli attivi e per fare le send / recv tra i rank inattivi che 
    devo svegliare e i rank attivi che devono svegliarli
2.0 -> Abbiamo due step:
    2.1 -> Check se rank inattivo è fallito 
        2.1.0 -> Come primo punto controlliamo se esistono rank inattivi 
        2.1.1 -> Poi per ogni rank fallito cerco una corrispondenza nell'array degli 
            inattivi se la trovo ritorno 1 altrimenti ritorno 0
    2.2 -> Sistema la struttura inactive_ranks: itero per ogni rank inattivo, e controllo
        se è fallito, se si incremento un contatore altrimenti riscrivo il rank nell'array 
        facendolo shiftare alla posizione (i - counter)
3.0 -> Per ogni rank fallito cerco una corrispondenza nell'array degli 
    attivi se la trovo ritorno 1 altrimenti ritorno 0
    3.1 -> check se numero di nodi attivi falliti <= numero di rank inattivi sopravvissuti
        3.1.0 -> Abbiamo blocchi di distance / 2 ranks che condividono gli stessi dati 
            per ogni blocco il primo rank che non è fallito e non ha i dati corrotti sarà il
            master del blocco ed è incaricato di svegliare gli inattivi e inviargli i dati 
            e di inviare i dati ai rank corrotti (sempre all'interno dello stesso blocco).
            Dati due rank r1 e r2 se r1 / distance == r2 / distance, significa che siamo 
            nello stesso blocco, prendo distance / 2, perchè sto considerando i fallimenti del
            passo precedente.
            Per i rank corrotti devo assicurarmi che non siano falliti altrimenti invio i 
            dati a un rank fallito che non li riceverà mai e rimango in attesa infinita
        3.1.1 -> Per ogni processo fallito calcolo il corrispettivo corrotto corr, se 
            corr == rank siamo noi i corrotti
        3.1.2 -> I processi inattivi da svegliare vengono presi dalla coda dei rank degli 
            inattivi, per cui ogni processo inattivo, calcola la sua posizione nell'array, 
            se la nostra posizione è >= (numero totale di processi inattivi - numero di processi 
            attivi falliti) allora ci aspettiamo di essere svegliati da un processo attivo
    3.2 -> Altrimenti se numero di nodi attivi falliti > numero di rank inattivi sopravvissuti
        3.2.0 -> Riduco alla potenza minore di due più vicina p' = 2**[log2 p], dove p è il numero
            attuale di processi attivi sopravvissutie e p' sarà il nuovo numero di processi attivi
        3.2.1 -> Calcolo il nuovo valore di distance d', dato che per passare da p a p'
            abbiamo ridotti la grandezza del comunicatore attivo di k, d' = d / k, 
            avremo quindi adesso d blocchi da d' rank che condividono gli stessi dati e rifaremo il passo d'.
            Per costruire il nuovo array degli attivi, itero su ogni rank attivo, e controllo non sia fallito,
            se ho ancora spazio nel blocco lo inserisco tra gli attivi, altrimenti lo metto tra gli inattivi
    3.3 -> Il vecchio comunicatore old_comm ha i rank relativi alla struttura attuale data, uso quindi
        quel comunicatore per prendermi il gruppo di tutti i rank, poi creo il gruppo dei sopravvissuti
        a partire da active ranks che sarà un sottogruppo del gruppo originale e alla fine creo il 
        nuovo comunicatore degli attivi 
    3.4 -> Se siamo precedentemente entrati nel passo 3.1 sarà necessario svegliare uno o più processi
        inattivi e inviare i dati sia a loro che ai rank che hanno dati corrotti, per cui in base alle 
        flag calcolate al passo 3.1 chi deve svegliare e inviare dati farà le Send ai relativi ranks, 
        mentre chi deve ricevere si metterà in attesa di ricevere i dati
4.0 -> Per ogni processo attivo itero sui processi falliti, se il rank è maggiore del rank del processo 
    fallito incremento un contatore k, alla fine decremento il valore del rank di k, in sintesi devo 
    calcolare il nuovo valore del rank nel nuovo comunicatore globale, decrementando ogni rank di 1 per 
    ogni processo fallito che ha rank minore, poi eseguo lo stesso procedimento per i processi inattivi
5.0 -> Per calcolare se sono un processo attivo o inattivo nel nuovo comunicatore globale, mi prendo il 
    rank relativo ad esso, poi vado a cercare se il mio rank è presente nell'array degli attivi, se si 
    setto il valore di active a 1 ed esco, altrimenti alla fine avrò settato 0
6.0 -> Se sei un rank inattivo, mettiti in attesa sulla recv di ricever il risultato finale, settiamo 
    il comunicatore globale come quello di riferimento per la chiamata e la flag MPI_ANY_SOURCE che 
    ci permetterà di ricevere da qualunque nodo attivo ci invierà il risultato
7.0 -> Se sei attivo e il tuo rank i relativo al comunicatore degli attivi è minore del numero totale 
    di rank inattivi allora farai la Send al rank i-esimo della struttura degli inattivi

## Edge cases
- Errori diversi da processo fallito 
    -> Quando intercetto un errore mi assicuro che l'errore sia l'intero 75, prima di 
        gestirlo, se dovesse essere diverso, aborto tutto il comunicatore globale
        (Warning: 75 non è un errore di MPI Standard, ma è MPIX_ERR_PROC_FAILED)
- Situazione in cui ho troppi fault per il passo in cui mi trovo 
    -> All'i-esimo passo avremo distance = 2^i; ovvero i rank a blocchi di distance size 
        hanno gli stessi dati per cui possiamo supportare failure a meno che non abbiamo 
        tutti i processi di un blocco che sono falliti o corrotti
    -> Possiamo estendere questa situazione, i blocchi comunicano a coppie ad esempio al passo 1
        con distance = 2 i primi due processi (0,1) comunicano con i processi (2,3), (4,5) con (6,7)
        e così via, questo significa che la comunicazione avviene a blocchi di distance * 2 dati, se 
        in un blocco di comunicazione abbiamo che tutti i processi sono falliti oppure hanno dati corrotti
        dovremmo far comunicare coloro che hanno i dati corrotti per avere i dati giusti, ma per 
        semplicità non considero questo caso e se succede aborto il programma.
        es: distance = 2; abbiamo P = 0,1,2,3; 0 <-> 2; 1 <-> 3; se falliscono 0 e 3, 2 e 1 avranno dati 
        corrotti, potremmo recuperare ancora il risultato finale facendo comunicare 1 <-> 2; ma per il 
        momento non considero questa casistica e siccome nel blocco di comunicazione che è fatto da 4 
        processi 2 sono falliti e 2 hanno dati corrotti, abortisco
- Failure all'interno dell'errhanlder
    -> Quando entro nell errhandler imposto la flag MPI_ERRORS_ARE_FATAL come errhandler, 
        se avvengono dei fault all'interno dell'errhandler tutti i processi vengono abortiti
        prima di uscire dall'errhandler reimposto la flag MPI_ERRORS_RETURN che mi permette 
        nuovamente di gestire gli errori che avvengono nella recursive doubling

## Generalizzazione con p non potenza di 2
- Calcola p' minore potenza di 2 piu vicina
- I rank >= p' si segnano come inattivi 
- rank inattivi mandano dati ai corrispettivi attivi: i-esimi inattivo 
    manda a i-esimo attivo, per come viene calcolato p' abbiamo che 
    numero di rank inattivi < numero di rank attivi

## Centralizzazione recovery blocco:
- Problema generale in svegliare rank che muore o mandare dati a rank corrotto, non sto considerando 
    il caso in cui colui che manderà i dati è corrotto e avrò dati sbagliati oppure è morto e non 
    esisterà il rank incaricato ergo cui non riceverò mai i miei dati o non verrà mai svegliato il 
    nuovo processo.
- Come risolvo?
    Centralizzo il processo, all'interno di un blocco di dati c'è almeno un processo che avrà i dati corretti 
    lo identifico e lui sarà incaricato di svegliare i processi inattivi e mandare i dati ai processi corrotti 
    per conto di quel blocco.
- Steps:
    1. Creo array block_masters di dimensione active_counts / d 
    2. Itero sugli attivi, e cerco il primo rank sano del blocco 
    3. Se sei un rank master itera sugli attivi
        3.1 se i è fallito e (i / d) == (rank / d) devi rimpiazzare i
        3.2 se i è corrotto e (i / d) == (rank / d) devi inviare dati corretti a i
    4. Se non sei un rank master devi calcolare se hai i dati corrotti 

## Osservazione
- Il grande problema è se abbiamo comportamenti in cui il programma si blocca oppure 
    termina con dati sbagliati, finchè negli edge cases abortiamo il programma poco male

## Problemi principali
- Errori Implementativi 
- Errori Algoritmici 
- Processi bloccati su send e recv 
- Processi che hanno errori sul comunicatore e abortano

## Testing
Idea:
- risultato esatto 
    - N == n di righe in mpi_output - killed
    - Tutti i processi hanno lo stesso risultato 
    - Il risultato è corretto 
- abort programma 
    - Cerca [MPI_ABORT, MPI_ERRORS_ARE_FATAL]
- risultato sbagliato 
    - N == n di righe in mpi_output - killed 
    - Processi hanno risultati diversi
    - Il risultato è sbagliato 
- errori [segfault] 
    - Cerca [Segmentation fault, (core dumped)]
- deadlock
    - Viene raggiunto il timeout

Steps:
- passare parametri da input 
    - n di processi * 
    - sleep time *
    - % killing * 
    - buf_size *
- Randomizzare i parametri * 
- Checker validità * 
- Salvare dati [log] *
- Migliorare Range randomici *
- Equazione per far durare run tra 5 e 10 sec *
- Analisi Statistiche *
- Testarlo sul cluster *

* mv log.csv ../../../../../../mnt/c/Users/lucam/Drive/Desktop
* Lanciarlo da src 
* rsync -vr micarelli_2061752@192.168.0.102:~/Fault-Tolerant/log2.csv .

## DEBUG ##
- Vedere quando si verificano segfault e deadlock 
- Usare dei log e debaggure per evidenziare i problemi
- Classificare i problemi riscontrati 
- Risolvere problemi 
- Testare di nuovo 

## GENERALIZZAZIONE RD ##
- Vedere implementazioni ufficiali
- Valida per tipi diversi
- Valida per operazioni diverse
- Considera commutatività / associatività

## Problema SendRecv
Quando un processo p muore dopo la barriera e prima della send recv, il suo partner non lo sa, 
quando va a fare la sendrecv rimane in attesa infinita.
Gli altri vanno avanti e quando arrivano alla barriera capiscono che qualcuno è morto il che dovrebbe 
tornare errore, ma siccome c'è un processo vivo bloccato, rimangono tutti ad aspettarlo.
Come posso risovlere?
-> Faccio un check prima della sendrecv che il mio partner è ancora vivo 
-> Problema ridotto a se la kill avviene direttamente nella SendRecv
* Può essere risolto usando la tecnica precedente e facendo direttamente le chiamate isend e irecv
    invece della sendrecv, il problema è che bisogna usare un timeout per le isend / irecv 
    ovvero io faccio le chiamate e mi blocco in un ciclo while per 10 volte e un totale di 0.1 secondi
    per assicurarmi che le chiamate riescano a completare, se lo faccio passando solo una flag sono sicuro
    che riesco a completarlo correttamente, ma se lo faccio direttamente sul buffer rischio che il tempo 
    di timeout che setto è troppo breve e i processi non riescono a completare le loro operazioni, questo
    behavioral porterebbe a una situazione in cui esco dal ciclo credendo che il mio partner sia morto e 
    che verrà notificato quando raggiungo la barriera, ma ciò non è vero, il mio partner è vivo e la barriera
    non notificherà nessun morto, questo porterebbe processi che hanno dati errati credendo siano stati 
    aggiustati correttamente.
    Bisognerebbe quindi scegliere un tempo di timeout adeguato, ma questo può diventare un fattore molto
    critico in quando non possiamo sapere quando tempo chiederà la chiamata sendrecv dato che dipende
    da molti fattori 
* Se il buffer è grande questo è un problema, ma si può risolvere divendendo il messaggio
    in chunck di dimensione fissa e fare tutto il lavoro di isend, irecv e riduzione step by step 
    oppure calcolare l'offset e fare la riduzione solo alla fine, in ogni caso se vengono fatte N 
    chiamate ne bast 1 in cui flag è settata a 0 e capiamo che il nostro partner e morto durante la
    comunicazione
* Purtroppo se il buffer è grande, gran parte della computazione viene spesa proprio nelle 
    sendrecv, quindi non è così raro che questo errore si verific 
* Un altro errore che accade in rari casi è dovuto a processi che notificano morti diverse,
    bisognerebbe approfondire come le chiamate ack/get_acked in ULFM sono realizzate 

## WRONG RESULT 
- Peggior caso, (silent error), se si verifica non me ne accorgo
- Si può verificare nelle implementazioni v2 e v3, quando notifico morte del partner che in 
    realtà non è morto
- Problema, come rilevo l'errore?
1. Probabilistico 
    --> eseguo una funzione hash su n elementi random nel buffer finale e verifico se l' hashcode 
        generato è uguale per tutti
        - Facile da implementare 
        - Posso avere collisioni
        - Overhead
2. Deterministico
    - Se rilevo partner morto durante lo step send/recv, mi devo assicurare che lo sia veramente
    1. Mi salvo il rank del partner morto r
    2. Al passo successivo devo avere error != 0
    3. Devo trovare r in rank_gc

## DEADLOCK
- Rilevamento del deadlock all'interno del programma
- Mi devo accorgere quando si sta verifando il deadlock 
- Si può verificare nella chiamate MPI che involvonno la comunicazione 
    tra più processi
- Idea: associo un timeout ad ogni chiamata, quando faccio la chamata lancio un thread che se non viene
    killato dopo N secondi chiama l'abort